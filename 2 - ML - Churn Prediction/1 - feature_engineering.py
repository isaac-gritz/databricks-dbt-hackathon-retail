# Databricks notebook source
# MAGIC %md
# MAGIC ## Feature Engineering
# MAGIC 1. This notebook leverages bamboolib for to interactively read data and develop ML features and then generate python pandas code
# MAGIC 1. We then use this code generated and the Pandas API on Spark to distribute the feature engineering using Spark

# COMMAND ----------

import pandas as pd
import numpy as np
from pyspark.sql.functions import *
from pyspark.sql.types import *
import bamboolib as bam

# COMMAND ----------

# MAGIC %md ### Read Delta Lake Tables Generated by dbt Cloud using bamboolib

# COMMAND ----------

bam

# COMMAND ----------

# read customer and sales order and product tables
dim_customers = spark.table("hive_metastore.dbx_dbt_retail.dim_customers").toPandas()
sales_orders = spark.table("hive_metastore.dbx_dbt_retail.fct_sales_orders").toPandas()
products = spark.table("hive_metastore.dbx_dbt_retail.stg_products").toPandas()

# COMMAND ----------

# MAGIC %md ### Create a Proxy for Churn Label - No Purchases in the Last 30 days

# COMMAND ----------

sales_orders

# COMMAND ----------

# Step: Keep rows where order_datetime < 2019-11-01 00:00:00
sales_orders = sales_orders.loc[sales_orders['order_datetime'] < '2019-11-01 00:00:00']

# Step: Group by and aggregate
churn = sales_orders.groupby(['dim_customer_id']).agg(order_datetime_max=('order_datetime', 'max')).reset_index()

# Step: Create new column 'days_no_purchases' from formula '(pd.to_datetime("2019-10-31")-churn['order_datetime_max']).dt.days'
churn['days_no_purchases'] = (pd.to_datetime("2019-10-31")-churn['order_datetime_max']).dt.days

# Step: Create new column 'churn' from formula '[1 if x > 30 else 0 for x in churn['days_no_purchases']]'
churn['churn'] = [1 if x > 30 else 0 for x in churn['days_no_purchases']]

# Step: Drop columns
churn = churn.drop(columns=['days_no_purchases', 'order_datetime_max'])

# Step: Drop duplicates based on ['dim_customer_id', 'churn']
churn = churn.drop_duplicates(keep='first')

# COMMAND ----------

# MAGIC %md
# MAGIC ### Customer Loyalty Program

# COMMAND ----------

dim_customers

# COMMAND ----------

# Step: Select columns
customer_loyalty = dim_customers[['customer_id', 'customer_name', 'loyalty_segment']]

# Step: Change data type of loyalty_segment to Integer
customer_loyalty['loyalty_segment'] = pd.to_numeric(customer_loyalty['loyalty_segment'], downcast='integer', errors='coerce')

# Step: Set values of loyalty_segment to 0 where loyalty_segment == 0 and otherwise to 1
tmp_condition = customer_loyalty['loyalty_segment'] == 0
customer_loyalty.loc[tmp_condition, 'loyalty_segment'] = 0
customer_loyalty.loc[~tmp_condition, 'loyalty_segment'] = 1

# COMMAND ----------

# MAGIC %md
# MAGIC ### Customer Buying Behavior

# COMMAND ----------

sales_orders

# COMMAND ----------

# Step: Group by and aggregate
customer_spend = sales_orders.groupby(['dim_customer_id']).agg(total_sum=('total', 'sum'), total_mean=('total', 'mean'), total_size=('total', 'size')).reset_index()

# Step: Rename multiple columns
customer_spend = customer_spend.rename(columns={'total_sum': 'order_total', 'total_mean': 'avg_order_total', 'total_size': 'num_orders'})

# COMMAND ----------

# MAGIC %md
# MAGIC ### Join Customer, Churn, and Buying Behavior Datasets

# COMMAND ----------

customer_loyalty

# COMMAND ----------

# Step: Inner Join with churn where customer_id=dim_customer_id
customer_churn = pd.merge(customer_loyalty, churn[['dim_customer_id', 'churn']], how='inner', left_on=['customer_id'], right_on=['dim_customer_id'])

# Step: Inner Join with customer_spend where customer_id=dim_customer_id
customer_churn = pd.merge(customer_churn, customer_spend[['dim_customer_id', 'order_total', 'avg_order_total', 'num_orders']], how='inner', left_on=['customer_id'], right_on=['dim_customer_id'])

# Step: Drop columns
customer_churn = customer_churn.drop(columns=['dim_customer_id_x', 'dim_customer_id_y'])

# Step: Rearranged the order of the columns
customer_churn = customer_churn[['churn'] + ['customer_id', 'customer_name', 'loyalty_segment', 'order_total', 'avg_order_total', 'num_orders']]

# COMMAND ----------

# MAGIC %md
# MAGIC ### Product Categories

# COMMAND ----------

sales_orders

# COMMAND ----------

# Step: Select columns
product_categories = sales_orders[['dim_customer_id', 'dim_product_id']]

# Step: Left Join with products where dim_product_id=product_id
product_categories = pd.merge(product_categories, products[['product_id', 'product_category']], how='left', left_on=['dim_product_id'], right_on=['product_id'])

# Step: Drop columns
product_categories = product_categories.drop(columns=['dim_product_id'])

# Step: Drop duplicates based on ['dim_customer_id', 'product_id', 'product_category']
product_categories = product_categories.drop_duplicates(keep='first')

# Step: Create new column 'index' from formula 'product_categories.index'
product_categories['index'] = product_categories.index

# Step: Pivot dataframe from long to wide format using the variable column 'product_category' and the value column 'index'
product_categories = product_categories.set_index(['dim_customer_id', 'product_id', 'product_category'])['index'].unstack(-1).reset_index()
product_categories.columns.name = ''

# Step: Replace missing values
product_categories = product_categories.fillna(0)

# Step: Change data type of ['Ankyo', 'Apson', 'Conan', 'Elpine', 'Karsair', 'Mannheiser', 'Mogitech', 'Mowepro', 'Olitscreens', 'Opple', 'Ramsung', 'Reagate', 'Rony', 'Sioneer', 'Zamaha'] to Boolean
for column_name in product_categories.select_dtypes(['float']).columns:
    product_categories[column_name] = product_categories[column_name].astype(bool)

# Step: Replace True with 1 in all columns
product_categories = product_categories.replace(True, 1)

# Step: Replace False with 0 in all columns
product_categories = product_categories.replace(False, 0)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Join to Create Final Dataset

# COMMAND ----------

customer_churn

# COMMAND ----------

# Step: Left Join with product_categories where customer_id=dim_customer_id
customer_churn = pd.merge(customer_churn, product_categories.drop(columns=['product_id']), how='left', left_on=['customer_id'], right_on=['dim_customer_id'])

# Step: Group by and aggregate
customer_churn = customer_churn.groupby(['customer_id']).agg({**{col: ['mean'] for col in ['churn_mean', 'order_total_mean', 'avg_order_total_mean', 'num_orders_mean', 'loyalty_segment_mean']}, **{col: ['sum'] for col in ['Ankyo', 'Apson', 'Conan', 'Elpine', 'Karsair', 'Mannheiser', 'Mogitech', 'Mowepro', 'Olitscreens', 'Opple', 'Ramsung', 'Reagate', 'Sioneer', 'Rony', 'Zamaha']}})
customer_churn.columns = ['_'.join(multi_index) for multi_index in customer_churn.columns.ravel()]
customer_churn = customer_churn.reset_index()

# Step: Change data type of loyalty_segment_mean_mean to Integer
customer_churn['loyalty_segment_mean_mean'] = pd.to_numeric(customer_churn['loyalty_segment_mean_mean'], downcast='integer', errors='coerce')

# Step: Change data type of num_orders_mean_mean to Integer
customer_churn['num_orders_mean_mean'] = pd.to_numeric(customer_churn['num_orders_mean_mean'], downcast='integer', errors='coerce')

# Step: Change data type of churn_mean_mean to Integer
customer_churn['churn_mean_mean'] = pd.to_numeric(customer_churn['churn_mean_mean'], downcast='integer', errors='coerce')

# Step: Rename multiple columns
customer_churn = customer_churn.rename(columns={'churn_mean_mean': 'churn', 'order_total_mean_mean': 'order_total', 'avg_order_total_mean_mean': 'avg_order_total', 'num_orders_mean_mean': 'num_orders', 'loyalty_segment_mean_mean': 'loyalty_segment', 'Ankyo_sum': 'Ankyo', 'Apson_sum': 'Apson', 'Conan_sum': 'Conan', 'Elpine_sum': 'Elpine', 'Karsair_sum': 'Karsair', 'Mannheiser_sum': 'Mannheiser', 'Mogitech_sum': 'Mogitech', 'Mowepro_sum': 'Mowepro', 'Olitscreens_sum': 'Olitscreens', 'Opple_sum': 'Opple', 'Ramsung_sum': 'Ramsung', 'Reagate_sum': 'Reagate', 'Sioneer_sum': 'Sioneer', 'Rony_sum': 'Rony', 'Zamaha_sum': 'Zamaha'})

# COMMAND ----------

# MAGIC %md
# MAGIC ### Distributing Feature Engineering Compute using Pandas API on Spark
# MAGIC * Minimal updates to the generated code to be able to execute at scale

# COMMAND ----------

## Import pandas on Spark API
import pyspark.pandas as ps

## Read Delta Lake Tables
# read customer and sales order and product tables
dim_customers = spark.table("hive_metastore.dbx_dbt_retail.dim_customers").pandas_api()
sales_orders = spark.table("hive_metastore.dbx_dbt_retail.fct_sales_orders").pandas_api()
products = spark.table("hive_metastore.dbx_dbt_retail.stg_products").pandas_api()


## Create a Proxy for Churn
# Step: Keep rows where order_datetime < 2019-11-01 00:00:00
sales_orders = sales_orders.loc[sales_orders['order_datetime'] < '2019-11-01 00:00:00']

# Step: Group by and aggregate
churn = sales_orders.groupby(['dim_customer_id']).agg(order_datetime_max=('order_datetime', 'max')).reset_index()

# Step: Create new column 'days_no_purchases' from formula '(ps.to_datetime("2019-10-31")-churn['order_datetime_max']).dt.days'
churn['days_no_purchases'] = (ps.to_datetime("2019-10-31")-(ps.to_datetime(churn['order_datetime_max'].dt.date)))/60/60/24

# Step: Create new column 'churn' from formula '[1 if x > 30 else 0 for x in churn['days_no_purchases']]'
churn['churn'] = churn['days_no_purchases'].apply(lambda x: 0 if x > 30 else 1)

# Step: Drop columns
churn = churn.drop(columns=['days_no_purchases', 'order_datetime_max'])

# Step: Drop duplicates based on ['dim_customer_id', 'churn']
churn = churn.drop_duplicates(keep='first')


## Customer Loyalty Program
# Step: Select columns
customer_loyalty = dim_customers[['customer_id', 'customer_name', 'loyalty_segment']]

# Step: Change data type of loyalty_segment to Integer
customer_loyalty['loyalty_segment'] = ps.to_numeric(customer_loyalty['loyalty_segment'], errors='coerce')

# Step: Set values of loyalty_segment to 0 where loyalty_segment == 0 and otherwise to 1
tmp_condition = customer_loyalty['loyalty_segment'] == 0
customer_loyalty.loc[tmp_condition, 'loyalty_segment'] = 0
customer_loyalty.loc[~tmp_condition, 'loyalty_segment'] = 1


## Customer Buying Behavior
# Step: Group by and aggregate
customer_spend = sales_orders.groupby(['dim_customer_id']).agg(total_sum=('total', 'sum'), total_mean=('total', 'mean'), total_size=('total', 'count')).reset_index()

# Step: Rename multiple columns
customer_spend = customer_spend.rename(columns={'total_sum': 'order_total', 'total_mean': 'avg_order_total', 'total_size': 'num_orders'})


## Join Customer, Churn, and Buying Behavior Datasets
# Step: Inner Join with churn where customer_id=dim_customer_id
customer_churn = ps.merge(customer_loyalty, churn[['dim_customer_id', 'churn']], how='inner', left_on=['customer_id'], right_on=['dim_customer_id'])

# Step: Inner Join with customer_spend where customer_id=dim_customer_id
customer_churn = ps.merge(customer_churn, customer_spend[['dim_customer_id', 'order_total', 'avg_order_total', 'num_orders']], how='inner', left_on=['customer_id'], right_on=['dim_customer_id'])

# Step: Drop columns
customer_churn = customer_churn.drop(columns=['dim_customer_id_x', 'dim_customer_id_y'])

# Step: Rearranged the order of the columns
customer_churn = customer_churn[['churn'] + ['customer_id', 'customer_name', 'loyalty_segment', 'order_total', 'avg_order_total', 'num_orders']]


## Product Categories
# Step: Select columns
product_categories = sales_orders[['dim_customer_id', 'dim_product_id']]

# Step: Left Join with products where dim_product_id=product_id
product_categories = ps.merge(product_categories, products[['product_id', 'product_category']], how='left', left_on=['dim_product_id'], right_on=['product_id'])

# Step: Drop columns
product_categories = product_categories.drop(columns=['dim_product_id'])

# Step: Drop duplicates based on ['dim_customer_id', 'product_id', 'product_category']
product_categories = product_categories.drop_duplicates(keep='first')

# Step: Create new column 'index' from formula 'product_categories.index'
product_categories['index'] = product_categories.index

# Step: Pivot dataframe from long to wide format using the variable column 'product_category' and the value column 'index'
product_categories = product_categories.set_index(['dim_customer_id', 'product_id', 'product_category'])['index'].unstack(-1).reset_index()
product_categories.columns.name = ''

# Step: Replace missing values
product_categories = product_categories.fillna(0)

# Step: Change data type of ['Ankyo', 'Apson', 'Conan', 'Elpine', 'Karsair', 'Mannheiser', 'Mogitech', 'Mowepro', 'Olitscreens', 'Opple', 'Ramsung', 'Reagate', 'Rony', 'Sioneer', 'Zamaha'] to Boolean
for column_name in product_categories.select_dtypes(['int']).columns:
    product_categories[column_name] = product_categories[column_name].astype(bool)

# Step: Replace True with 1 in all columns and False with 0 for all columns
for column_name in product_categories.select_dtypes(['bool']).columns:
    product_categories[column_name] = product_categories[column_name].astype('int')

## Join to Create Final Dataset
# Step: Left Join with product_categories where customer_id=dim_customer_id
customer_churn = ps.merge(customer_churn, product_categories.drop(columns=['product_id']), how='left', left_on=['customer_id'], right_on=['dim_customer_id'])

# Step: Group by and aggregate
customer_churn = customer_churn.groupby(['customer_id']).agg({**{col: ['mean'] for col in ['churn', 'order_total', 'avg_order_total', 'num_orders', 'loyalty_segment']}, **{col: ['sum'] for col in ['Ankyo', 'Apson', 'Conan', 'Elpine', 'Karsair', 'Mannheiser', 'Mogitech', 'Mowepro', 'Olitscreens', 'Opple', 'Ramsung', 'Reagate', 'Sioneer', 'Rony', 'Zamaha']}})
customer_churn.columns = ['_'.join(multi_index) for multi_index in customer_churn.columns.ravel()]
customer_churn = customer_churn.reset_index()

# Step: Change data type of loyalty_segment_mean to Integer
customer_churn['loyalty_segment_mean'] = customer_churn['loyalty_segment_mean'].astype('int')

# Step: Change data type of num_orders_mean to Integer
customer_churn['num_orders_mean'] = customer_churn['num_orders_mean'].astype('int')

# Step: Change data type of churn_mean to Integer
customer_churn['churn_mean'] = ps.to_numeric(customer_churn['churn_mean'], errors='coerce')
customer_churn['churn_mean'] = customer_churn['churn_mean'].astype('int')

# Step: Rename multiple columns
customer_churn = customer_churn.rename(columns={'churn_mean': 'churn', 'order_total_mean': 'order_total', 'avg_order_total_mean': 'avg_order_total', 'num_orders_mean': 'num_orders', 'loyalty_segment_mean': 'loyalty_segment', 'Ankyo_sum': 'Ankyo', 'Apson_sum': 'Apson', 'Conan_sum': 'Conan', 'Elpine_sum': 'Elpine', 'Karsair_sum': 'Karsair', 'Mannheiser_sum': 'Mannheiser', 'Mogitech_sum': 'Mogitech', 'Mowepro_sum': 'Mowepro', 'Olitscreens_sum': 'Olitscreens', 'Opple_sum': 'Opple', 'Ramsung_sum': 'Ramsung', 'Reagate_sum': 'Reagate', 'Sioneer_sum': 'Sioneer', 'Rony_sum': 'Rony', 'Zamaha_sum': 'Zamaha'})

# Step: Change data type of ['Ankyo', 'Apson', 'Conan', 'Elpine', 'Karsair', 'Mannheiser', 'Mogitech', 'Mowepro', 'Olitscreens', 'Opple', 'Ramsung', 'Reagate', 'Sioneer', 'Rony', 'Zamaha'] to Boolean
for column_name in ['Ankyo', 'Apson', 'Conan', 'Elpine', 'Karsair', 'Mannheiser', 'Mogitech', 'Mowepro', 'Olitscreens', 'Opple', 'Ramsung', 'Reagate', 'Sioneer', 'Rony', 'Zamaha']:
    customer_churn[column_name] = customer_churn[column_name].astype(bool)

# Step: Change data type of ['Ankyo', 'Apson', 'Conan', 'Elpine', 'Karsair', 'Mannheiser', 'Mogitech', 'Mowepro', 'Olitscreens', 'Opple', 'Ramsung', 'Reagate', 'Sioneer', 'Rony', 'Zamaha'] to Integer
for column_name in ['Ankyo', 'Apson', 'Conan', 'Elpine', 'Karsair', 'Mannheiser', 'Mogitech', 'Mowepro', 'Olitscreens', 'Opple', 'Ramsung', 'Reagate', 'Sioneer', 'Rony', 'Zamaha']:
    customer_churn[column_name] = customer_churn[column_name].astype('int')

# COMMAND ----------

# MAGIC %md
# MAGIC ### Save Result to Delta Lake Table

# COMMAND ----------

# MAGIC %sql
# MAGIC DROP TABLE IF EXISTS hive_metastore.dbx_dbt_retail.churn_prediction

# COMMAND ----------

customer_churn.to_table(name = "hive_metastore.dbx_dbt_retail.churn_prediction", format = "delta", mode = "overwrite")

# COMMAND ----------

# MAGIC %sql
# MAGIC USE hive_metastore.dbx_dbt_retail;
# MAGIC 
# MAGIC SELECT * FROM churn_prediction;

# COMMAND ----------

# MAGIC %md
# MAGIC Now let's create an AutoML model in the [`Experiments tab`](#mlflow/experiments)
# MAGIC 
# MAGIC * [`AutoML Experiment`](https://uc-aug.cloud.databricks.com/?o=42982497019954#mlflow/experiments/2916963709048647/)
# MAGIC * [`AutoML Data Exploration`](https://uc-aug.cloud.databricks.com/?o=42982497019954#notebook/2916963709048664/command/2916963709048665)
# MAGIC * [`AutoML Best Run`](https://uc-aug.cloud.databricks.com/?o=42982497019954#notebook/2916963709052620/command/2916963709052621)
# MAGIC * [`AutoML Model Artifact`](https://uc-aug.cloud.databricks.com/?o=42982497019954#mlflow/experiments/2916963709048647/runs/0df78395355643f19e093e9bd300b83b/artifactPath/model)
# MAGIC * [`AutoML Registered Model`](https://uc-aug.cloud.databricks.com/?o=42982497019954#mlflow/models/dbx_dbt_churn_prediction_decision_tree/versions/2)
# MAGIC * [`AutoML Batch Scoring`](https://uc-aug.cloud.databricks.com/?o=42982497019954#notebook/1543362028797696/command/1543362028797697)
# MAGIC * [`Churn Prediction Dashboard`](https://uc-aug.cloud.databricks.com/sql/dashboards/a438998a-eeb1-41f8-8eeb-37a9100b42b5-churn-prediction-dashboard?edit&o=42982497019954)
# MAGIC * [`Customer 360 Dashboard`](https://uc-aug.cloud.databricks.com/sql/dashboards/b554ec10-3761-4982-bddd-52cd34fc8890-customer-360?o=42982497019954)
